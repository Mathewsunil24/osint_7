# ğŸ•µï¸â€â™‚ï¸ OSINT Lab 7 - Automated Social Media OSINT Pipeline

## ğŸ“˜ Project Overview
This project implements an **automated OSINT (Open Source Intelligence) pipeline** that collects, analyzes, and visualizes social media data from multiple platforms such as **Reddit** and **Twitter**.  
It performs sentiment analysis, generates word clouds, and stores results for further investigation.

---

## ğŸš€ Features
- ğŸŒ Multi-platform data collection (Reddit, Twitter)
- ğŸ§¹ Automated data cleaning and preprocessing
- ğŸ’¬ Sentiment analysis using TextBlob
- ğŸ“Š Data visualization with word clouds and sentiment charts
- ğŸ’¾ Local data storage in JSON format
- ğŸ”„ Modular structure for easy updates and debugging

---

## ğŸ§° Project Structure
osint_pipeline/
â”œâ”€â”€ collectors/ # Reddit and Twitter data collectors
â”‚ â”œâ”€â”€ reddit_collector.py
â”‚ â”œâ”€â”€ twitter_collector.py
â”‚
â”œâ”€â”€ utils/ # Processing and visualization scripts
â”‚ â”œâ”€â”€ data_cleaner.py
â”‚ â”œâ”€â”€ visualize.py
â”‚
â”œâ”€â”€ data/ # Contains collected and processed data
â”‚ â”œâ”€â”€ reddit_results.json
â”‚ â”œâ”€â”€ twitter_results.json
â”‚ â”œâ”€â”€ reddit_wordcloud.png
â”‚ â”œâ”€â”€ reddit_sentiment.png
â”‚ â”œâ”€â”€ twitter_wordcloud.png
â”‚ â”œâ”€â”€ twitter_sentiment.png
â”‚
â”œâ”€â”€ main.py # Main pipeline file
â”œâ”€â”€ requirements.txt # Dependencies list
â””â”€â”€ README.md # Project documentation

yaml
Copy code

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Mathewsunil24/osint_7.git
cd osint_pipeline
2ï¸âƒ£ Create a Virtual Environment
bash
Copy code
python -m venv osint_env
osint_env\Scripts\activate   # On Windows
# or
source osint_env/bin/activate   # On Linux/Mac
3ï¸âƒ£ Install Dependencies
bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Set Up Environment Variables
Create a .env file in the root folder and add your API keys:

ini
Copy code
REDDIT_CLIENT_ID=your_client_id
REDDIT_SECRET=your_secret
REDDIT_USER_AGENT=your_user_agent

TWITTER_BEARER_TOKEN=your_bearer_token
5ï¸âƒ£ Run the Pipeline
bash
Copy code
python main.py
ğŸ”‘ API Keys Required
Reddit API: Client ID, Secret, and User Agent

Twitter API: Bearer Token

(Optional) You can reduce the number of tweets fetched in twitter_collector.py to avoid rate-limit issues.

ğŸ§  How It Works
Data Collection â€“ Collects posts and tweets using Reddit and Twitter APIs.

Processing â€“ Cleans text, removes duplicates and unwanted symbols.

Sentiment Analysis â€“ Classifies content as Positive, Negative, or Neutral using TextBlob.

Visualization â€“ Generates:

ğŸ“ˆ Sentiment distribution chart

â˜ï¸ WordCloud for top words

Storage â€“ Saves outputs in /data folder for review.

ğŸ“¸ Sample Outputs
Platform	Word Cloud	Sentiment Graph
Reddit		
Twitter	

ğŸ§© Technologies Used
ğŸ Python 3

ğŸŒ PRAW (Reddit API Wrapper)

ğŸ¦ Tweepy (Twitter API Wrapper)

ğŸ§  TextBlob for sentiment analysis

ğŸ¨ Matplotlib & WordCloud for visualization

ğŸ’¾ JSON & SQLite for data handling
